# -*- coding: utf-8 -*-
"""Intro to kNNs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15_IVnL4dnD8E5h6UsDa9PoQO_eRPtSSX

<h1>What is kNN?</h1>
<p>kNN stands for k Nearest Neighbors
<p>It is a <b>supervised</b> learning algorithm (my video explaining supervised vs unsupervised: https://youtu.be/2Z1B0xESzMw </p>
    <p>But in essence <b>supervised</b> simply means that we require <b>labelled</b> data</p>
    <p>It can solve both <b>classification</b> and <b>regression</b> problems.
<p>A <b>classification</b> problem describes data with a discrete / categorical response variable -- e.g. smoker + age > 60 --> 5 on the risk scale</p>
<p>A <b>regression</b> problem describes data with a real numeric response variable e.g. 3 bedroom house + los angeles + 1400 square foot = $1.23 million</p>
<p>kNN assumption: "similar things exist near each other". This is similar to the CBR assumption that the world is regular and similar problems have similar solutions. video on CBR https://youtu.be/Iy2gO8svdMI </p>
"""

#sklearn is a machine learning library in python
from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier, KNeighborsRegressor

"""<h2>How does kNN work?</h2>
<p>A number of neighbors k is chosen e.g. 10</p>
<p>Then for each data point p, the 10 data points whose input variables (X) are most <b>similar</b> to p (based on some distance similarity metric) are selected</p>
<p>Then the <b>average</b> output variable of these 10 points are found and used as the predicted output value for p</p>
<p>If this is a <b>classification</b> task then the averaging might be majority voting</p>
<p>For a <b>regression</b> task it could be simply getting the mean of the 10 outputs</p>
<p>The averaging can also include <b>weighting</b> by distance from p so closer points have more of a contribution in the final value</p>
"""

from sklearn.datasets import load_iris

iris_data=load_iris()

iris_data.data

iris_data.target

"""<h2>Underfitting and Overfitting</h2>
<p><b>Underfitting</b> occurs when there isn't enough data to learn the underlying features and make accurate predictions</p>
<p><b>Overfitting</b> occurs when the model fits the training data too well and does not work well for unseen data</p>

<h3>Solution: Validation</h3>
<p>The data is split into training and testing -- e.g. the model is trained on 80%, and tested on 20%</p>
<p>Or even better, training, validation and testing -- e.g. the model is trained on 60%, adjusted on 20% and tested on 20%</p>
<h4>More Advanced: Cross-Validation</h4>
<p><b>Leave one out: </b>For each data point p, use p as testing, and the rest as training. Average.</p>
<p><b>k-Fold Cross Validation: </b>Split the data into k folds, for k runs, use one fold as testing and the rest as training. Average over the k folds. (We will use this one)</p>
"""

from sklearn.model_selection import cross_val_predict

X=iris_data.data
y=iris_data.target
knn = KNeighborsClassifier(n_neighbors=10)
y_pred = cross_val_predict(knn, X, y, cv=5)

y_pred

"""<h2>Evaluation</h2>
<p><b>Mean Squared Error: </b>averaged of the squared error of the difference between the actual and predicted values (lower = better)
<p><b>R2: </b>the correlation between the dependent variable and the set of independent variables (higher = better)
"""

from sklearn.metrics import mean_squared_error, r2_score

print(mean_squared_error(y,y_pred))
print(r2_score(y,y_pred))

"""<h2>What value for k?</h2>
<p>Test a range of values and see which produces the lowest error</p>
"""

error = []
for k in range(1,51):
    knn = KNeighborsClassifier(n_neighbors=k)
    y_pred = cross_val_predict(knn, X, y, cv=5)
    error.append(mean_squared_error(y,y_pred))

import matplotlib.pyplot as plt

plt.plot(range(1,51),error)

from sklearn.datasets import load_boston

boston_data=load_boston()

boston_data.data

boston_data.target

X=boston_data.data
y=boston_data.target
knn = KNeighborsRegressor(n_neighbors=10)
y_pred = cross_val_predict(knn, X, y, cv=5)

y_pred

from math import sqrt

print(sqrt(mean_squared_error(y,y_pred)))
print(r2_score(y,y_pred))

error = []
for k in range(1,51):
    knn = KNeighborsRegressor(n_neighbors=k)
    y_pred = cross_val_predict(knn, X, y, cv=5)
    error.append(mean_squared_error(y,y_pred))

plt.plot(range(1,51),error)

"""<h2>Scaling</h2>
<p>With numeric variables, scaling can help to reduce outliers.
    <p>A standard scaler, transforms variables into a value related to the min, max of the variable</p>
    <p>Scaling must be done on the training data, and then applied to the testing, so we need to add this to our classifier pipeline</p>
"""

from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

classifier_pipeline = make_pipeline(StandardScaler(), KNeighborsRegressor(n_neighbors=10))

y_pred = cross_val_predict(classifier_pipeline, X, y, cv=5)
print(sqrt(mean_squared_error(y,y_pred)))
print(r2_score(y,y_pred))

error = []
for k in range(1,51):
    classifier_pipeline = make_pipeline(StandardScaler(), KNeighborsRegressor(n_neighbors=k))
    y_pred = cross_val_predict(classifier_pipeline, X, y, cv=5)
    error.append(mean_squared_error(y,y_pred))

plt.plot(range(1,51),error)

"""<h3>Comparing Models</h3>
<p>It's important when you compare models that you are comparing on the same data.
    <p>When the data is split into training and testing using cross-validation, you need to split the same way every time if you want to really compare trials</p>
"""

from sklearn.model_selection import KFold

cv = KFold(n_splits=5, random_state=0, shuffle=False)

classifier_pipeline = make_pipeline(StandardScaler(), KNeighborsRegressor(n_neighbors=10))
y_pred = cross_val_predict(classifier_pipeline, X, y, cv=cv)
print(sqrt(mean_squared_error(y,y_pred)))
print(r2_score(y,y_pred))

"""<h3>Which is better? kNN or a simple linear regression model?</h3>
<p>Looking at the difference in RMSE and R2 we might assume that the knn performs significantly better than the Linear Model, however a two-sample t-test shows that they are not significantly different.
"""

from sklearn.linear_model import LinearRegression

classifier_pipeline = make_pipeline(StandardScaler(), LinearRegression())
y_pred2 = cross_val_predict(classifier_pipeline, X, y, cv=cv)
print(sqrt(mean_squared_error(y,y_pred2)))
print(r2_score(y,y_pred2))

diff_knn = [abs(round(y[i]-y_pred[i],2)) for i in range(0,len(y))]
diff_linear = [abs(round(y[i]-y_pred2[i],2)) for i in range(0,len(y))]

from scipy.stats import ttest_ind
ttest_ind(diff_knn,diff_linear)

"""<h3>Feature Selection</h3>
<p>Sometimes not all input variables are good predictor of the output variables. Sometimes, some of them can actually decrease the accuracy of the model.</p>
<p>Feature Selection allows you to try out the model with different subsets of input variables to determine which are the best predictors</p>

<h3>Evaluation</h3>
<p>Looking at the mean squared error, and r2 score alone, often we can think we have a great model when in actual fact there are some underlying problems.
<p>That is why it is always imporant to look at the <b>residuals</b>
    <p><b>Coming soon: </b>Feature selection and Evaluation for kNN Regression (and most learning algorithms tbh)
"""

